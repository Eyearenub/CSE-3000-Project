# ------------------------------------------------------------------------------
# bias_analysis.py  –  Local, no‑frills evaluation + bias audit
# ------------------------------------------------------------------------------
# • Expects the three files generated by your predictor in the **same folder**:
#     predicted_felonys_2025.csv        ← note: typo kept for compatibility
#     predicted_misdemeanors_2025.csv
#     predicted_violations_2025.csv
# • Outputs
#     metrics_classification_report.csv          precision/recall/F1 per class
#     metrics_confusion_matrix.csv               3×3 confusion matrix
#     bias_fpr_by_race.csv   +  plot_fpr_race.png
#     bias_fpr_by_borough.csv +  plot_fpr_borough.png
# ------------------------------------------------------------------------------
# Dependencies (install once):
#     pip install pandas scikit-learn matplotlib
# ------------------------------------------------------------------------------

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
from sklearn.metrics import classification_report, confusion_matrix

FILES = {
    'FELONY':       'predicted_felonys_2025.csv',         # original filename has a typo
    'MISDEMEANOR':  'predicted_misdemeanors_2025.csv',
    'VIOLATION':    'predicted_violations_2025.csv'
}

INCOME_MAP = {
    'BRONX': 45000,
    'BROOKLYN': 60000,
    'MANHATTAN': 86000,
    'QUEENS': 72000,
    'STATEN ISLAND': 82000
}

# -------------------------------------------------------------------
# 1) Load & merge the three prediction files
# -------------------------------------------------------------------

def load_predictions():
    frames = []
    for label, fname in FILES.items():
        if not os.path.isfile(fname):
            raise FileNotFoundError(f"Required file '{fname}' not found in working dir.")
        df = pd.read_csv(fname)
        if 'predicted_label' not in df.columns:
            df['predicted_label'] = label  # fall‑back if column missing
        frames.append(df)
    return pd.concat(frames, ignore_index=True)

# -------------------------------------------------------------------
# 2) Global accuracy – classification report & confusion matrix
# -------------------------------------------------------------------

def save_metrics(df):
    y_true, y_pred = df['law_cat_cd'], df['predicted_label']

    report = classification_report(y_true, y_pred, output_dict=True)
    report_df = pd.DataFrame(report).transpose()
    report_df.to_csv('metrics_classification_report.csv', index_label='Class')
    print("✔︎ metrics_classification_report.csv written")

    cm = confusion_matrix(y_true, y_pred, labels=list(FILES.keys()))
    cm_df = pd.DataFrame(cm,
                         index=[f"Actual {c.title()}" for c in FILES.keys()],
                         columns=[f"Pred {c.title()}" for c in FILES.keys()])
    cm_df.to_csv('metrics_confusion_matrix.csv')
    print("✔︎ metrics_confusion_matrix.csv written")

# -------------------------------------------------------------------
# 3) Bias: False‑Positive Rate by victim race & by borough
# -------------------------------------------------------------------

def fpr(group):
    fp = ((group['predicted_label'] == 'FELONY') & (group['law_cat_cd'] != 'FELONY')).sum()
    tn = ((group['predicted_label'] != 'FELONY') & (group['law_cat_cd'] != 'FELONY')).sum()
    return fp / (fp + tn) if (fp + tn) else np.nan


def bias_by_race(df):
    race_counts = df['vic_race'].value_counts()
    major_races = race_counts[race_counts >= 200].index  # ignore small samples

    rows = []
    for race in major_races:
        grp = df[df['vic_race'] == race]
        rows.append({'Victim Race': race, 'Count': len(grp), 'FPR_Felony': fpr(grp)})

    race_df = pd.DataFrame(rows).sort_values('FPR_Felony', ascending=False)
    race_df.to_csv('bias_fpr_by_race.csv', index=False)
    print("✔︎ bias_fpr_by_race.csv written")
    _bar(race_df, 'Victim Race', 'FPR_Felony', 'False‑Positive Rate (Felony) by Victim Race',
         'plot_fpr_race.png')


def bias_by_borough(df):
    rows = []
    for boro, inc in INCOME_MAP.items():
        grp = df[df['boro_nm'] == boro]
        if grp.empty:  # skip if no records
            continue
        rows.append({'Borough': boro, 'Median_Income': inc, 'Count': len(grp), 'FPR_Felony': fpr(grp)})

    boro_df = pd.DataFrame(rows).sort_values('FPR_Felony', ascending=False)
    boro_df.to_csv('bias_fpr_by_borough.csv', index=False)
    print("✔︎ bias_fpr_by_borough.csv written")
    _bar(boro_df, 'Borough', 'FPR_Felony', 'False‑Positive Rate (Felony) by Borough',
         'plot_fpr_borough.png')

# -------------------------------------------------------------------
# 4) Simple bar‑plot helper
# -------------------------------------------------------------------

def _bar(df, xcol, ycol, title, outfile):
    plt.figure(figsize=(8,5))
    plt.bar(df[xcol], df[ycol])
    plt.ylabel(ycol)
    plt.title(title)
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.savefig(outfile, dpi=150)
    plt.close()
    print(f"  ↳ saved {outfile}")


# -------------------------------------------------------------------
# 5) Disparate Impact
# -------------------------------------------------------------------

def disparate_impact(df, group_col, positive_label='FELONY', min_rows=200):
    # 1) Keep only rows with enough data for the group
    vc = df[group_col].value_counts()
    eligible = vc[vc >= min_rows].index
    
    # 2) Compute selection rate per group
    rows = []
    for g in eligible:
        gdf = df[df[group_col] == g]
        sel_rate = (gdf['predicted_label'] == positive_label).mean()
        rows.append({'Group': g, 'N': len(gdf), 'SelectionRate': sel_rate})
    
    di = pd.DataFrame(rows)
    
    # 3) Pick the *most favored* group as reference (lowest adverse‑outcome rate)
    ref_rate = di['SelectionRate'].min()  # if predicting *adverse* labels
    di['ImpactRatio'] = di['SelectionRate'] / ref_rate
    
    # 4) Flag potential adverse impact
    di['<80% Rule Violated?'] = di['ImpactRatio'] < 0.80
    
    di.to_csv(f'disparate_impact_{group_col}.csv', index=False)
    print(f'✔︎ disparate_impact_{group_col}.csv written')
    return di

# -------------------------------------------------------------------
# 6) Main routine
# -------------------------------------------------------------------

def main():
    df = load_predictions()
    print(f"Merged dataset: {len(df):,} rows")

    save_metrics(df)
    bias_by_race(df)
    bias_by_borough(df)
    disparate_impact(df, 'vic_race')
    disparate_impact(df, 'boro_nm')
    print("All done ✔︎  Check the generated CSVs & PNG plots.")


if __name__ == "__main__":
    main()
